_wandb:
    value:
        cli_version: 0.21.0
        e:
            mj3rfxf90dc6hcxguvp093t8dpwbkqz5:
                args:
                    - --config
                    - /workspace/demo/Odyssey/configs/FINAL_MLM_SMALL_TEST.yaml
                    - --use-wandb
                codePath: odyssey/train/train_fsdp.py
                codePathLocal: train_fsdp.py
                cpu_count: 112
                cpu_count_logical: 224
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "32212254720"
                        used: "775004160"
                email: connor@anthrogen.com
                executable: /usr/bin/python
                git:
                    commit: 018d8d45631d6e17c23884c9aa525ba9b0e15367
                    remote: git@github.com:Anthrogen/Odyssey.git
                gpu: NVIDIA H100 80GB HBM3
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-cb23291d-047a-ca52-731c-68fb44f6a50f
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-66141653-4f7e-f5c5-fa0a-ae4e73270226
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-d1a5c4d9-4200-5a4d-fe5e-3a1fd97cba40
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-7a7190dc-9bd5-df89-ffce-0d6918dd87a7
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-a6a0e6b3-3d40-e8d7-0b0b-9d2867f7d47d
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-169f4fdb-840d-d824-bfcc-1c3a46dd56d3
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-21d89e55-f6d9-0e12-7063-aee961d32b3b
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-6562a925-c367-5dc0-1647-5c5e28f88eec
                host: 24886fa5a5aa
                memory:
                    total: "2164093640704"
                os: Linux-6.8.0-59-generic-x86_64-with-glibc2.35
                program: /workspace/demo/Odyssey/odyssey/train/train_fsdp.py
                python: CPython 3.11.11
                root: /workspace/demo/Odyssey/odyssey/train
                startedAt: "2025-08-07T01:46:19.683070Z"
                writerId: mj3rfxf90dc6hcxguvp093t8dpwbkqz5
        m:
            - "1": model/val_rmsd
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": step
              "6":
                - 3
              "7": []
            - "1": model/val_loss_struct
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": model/val_seq_acc
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": model/val_struct_acc
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/learning_rate
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_rmsd
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_seq_acc
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_loss_seq
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_loss_struct
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_struct_acc
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": model/val_loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": model/val_loss_seq
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/train_loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": batch/epoch
              "5": 2
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.11.11
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 7
                - 15
                - 16
            "4": 3.11.11
            "5": 0.21.0
            "12": 0.21.0
            "13": linux-x86_64
checkpoint_save_freq:
    value: 10000
model_0_batch_size:
    value: 4
model_0_d_model:
    value: 256
model_0_first_block:
    value: Self Consensus
model_0_mask_config:
    value: simple
model_0_max_epochs:
    value: 150
model_0_max_len:
    value: 2048
model_0_n_heads:
    value: 8
model_0_n_layers:
    value: 12
model_0_style:
    value: mlm
num_models:
    value: 1
world_size:
    value: 8
