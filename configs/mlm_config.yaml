# Odyssey Default Configuration File
# Transformer Trunk MLM Training Configuration

# =============================================================================
# Model Configuration
# =============================================================================
model_cfg:
  trunk_cfg:
    style: "mlm"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
    d_model: 128
    n_heads: 1
    n_layers: 3
    max_len: 2048
    dropout: 0.1
    ff_mult: 4
    reference_model_seed: 42
    fsq_encoder_path: "/workspace/demo/Odyssey/checkpoints/fsq/SA_stage_1_simple_model.pt"
    
    # First block configuration
    first_block_cfg:
      # self_consensus_cfg:  # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
      #   consensus_num_iterations: 1
      #   consensus_connectivity_type: "local_window"  # Options: "local_window", "scored_window"
      #   consensus_w: 2
      #   consensus_r: 24
      #   consensus_edge_hidden_dim: 12
      self_attention_cfg: {}
      # geometric_attention_cfg: {}
      # reflexive_attention_cfg: {}

# =============================================================================
# Training Configuration
# =============================================================================
train_cfg:
  training_cfg:
    batch_size: 4
    max_epochs: 50
    learning_rate: 0.00001
    data_dir: "/workspace/demo/Odyssey/sample_data/1k.csv"
    checkpoint_dir: "/workspace/demo/Odyssey/checkpoints/transformer_trunk"
    
    # Loss configuration
    loss_config:
      cross_entropy_loss_cfg:
        seq_loss_weight: 1.0
        struct_loss_weight: 1.0
        loss_elements: "masked"  # Different loss element strategy
    
    # Masking configuration
    mask_config:
      # simple_mask_cfg:
      #   mask_prob_seq: 0.2
      #   mask_prob_struct: 0.2
      complex_mask_cfg: {}
