# Configuration for MLM with Reflexive Attention
# Example of MLM training with different attention mechanism

model_cfg:
  type: "trunk_cfg"
  params:
    style: "mlm"
    d_model: 512
    n_heads: 8
    n_layers: 10
    max_len: 2048
    dropout: 0.1
    ff_mult: 4
    reference_model_seed: 42
    
    # Using Reflexive Attention
    first_block_cfg:
      type: "reflexive_attention_cfg"
      params: {}
    
    fsq_encoder_path: "checkpoints/fsq/GA_stage_1_iter1_simple.pt"

train_cfg:
  type: "training_cfg"
  params:
    batch_size: 16
    max_epochs: 80
    learning_rate: 3e-5
    data_dir: "sample_data/1k"
    checkpoint_dir: "checkpoints/transformer_trunk"
    
    loss_config:
      type: "cross_entropy_loss_cfg"
      params:
        seq_loss_weight: 1.0
        struct_loss_weight: 1.0
        loss_elements: "non_beospank"  # Different loss element strategy
    
    mask_config:
      type: "simple_mask_cfg"
      params:
        mask_prob_seq: 0.25    # Higher masking probability
        mask_prob_struct: 0.25