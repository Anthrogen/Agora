# Odyssey Configuration File
# This file contains configuration parameters matching the configurations.py structure

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Model style determines the type of model and training approach
  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
  style: "stage_1"
  
  # Core transformer parameters
  d_model: 768              # Model dimension (hidden size)
  n_heads: 12               # Number of attention heads
  n_layers: 12              # Number of transformer layers
  max_len: 2048             # Maximum sequence length
  dropout: 0.1              # Dropout rate
  ff_mult: 4                # Feedforward multiplier (ff_dim = d_model * ff_mult)
  
  # Block type for first transformer block
  # Options: "self_attention", "geometric_attention", "reflexive_attention", "self_consensus"
  block_type: "self_consensus"
  
  # Block-specific parameters
  block_params:
    # Self-Consensus parameters (used when block_type is "self_consensus")
    num_iterations: 1       # Number of consensus gradient iterations
    connectivity_type: "local_window"  # "local_window" or "top_w"
    w: 2                    # Window size for local_window, or number of connections for top_w
    r: 24                   # Rank of Lambda_ij matrices
    edge_hidden_dim: 12     # Hidden dimension for edge networks
  
  # FSQ parameters (used for stage_1 and stage_2)
  fsq:
    latent_dim: 32          # Pre-quantized continuous latent dimension
    levels: [7, 5, 5, 5, 5] # Quantization levels for each dimension
    # encoder_path: null    # Path to pre-trained encoder (required for stage_2)
  
  # FSQ encoder path for trunk models (mlm, discrete_diffusion)
  # fsq_encoder_path: "checkpoints/fsq/GA_stage_1_iter1_simple.pt"

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Basic training parameters
  batch_size: 32
  max_epochs: 100
  learning_rate: 1e-4
  
  # Data paths
  data_dir: "sample_data/1k"
  checkpoint_dir: "checkpoints"

# =============================================================================
# Loss Configuration
# =============================================================================
loss:
  # Loss type: "cross_entropy", "kabsch_rmsd", or "diffusion"
  type: "cross_entropy"
  
  # Cross-entropy loss weights
  weights:
    sequence: 1.0
    structure: 1.0
  
  # Loss elements - which positions contribute to loss
  # Options: "masked", "non_beospank", "non_special"
  loss_elements: "masked"
  
  # RMSD elements (for kabsch_rmsd loss)
  # Options: "masked", "non_beospank", "non_special", "non_masked"
  # rmsd_elements: "non_masked"

# =============================================================================
# Masking Configuration
# =============================================================================
masking:
  # Masking strategy: "simple", "complex", "discrete_diffusion", "none"
  strategy: "simple"
  
  # Simple masking parameters
  simple:
    mask_prob_seq: 0.2      # Probability of masking sequence tokens
    mask_prob_struct: 0.2   # Probability of masking structure tokens
  
  # Complex masking parameters
  # complex:
  #   # Add parameters when ComplexMaskConfig is fully defined
  
  # Discrete diffusion parameters
  discrete_diffusion:
    noise_schedule: "linear"  # "linear", "inverted_u", or "uniform"
    sigma_min: 0.31
    sigma_max: 5.68
    num_timesteps: 100

# =============================================================================
# Experiment Configuration (for reference, not used in core configs)
# =============================================================================
experiment:
  name: "default_experiment"
  seed: 42
  device: "cuda"