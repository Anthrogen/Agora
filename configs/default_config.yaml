# Odyssey Default Configuration File
# FSQ Stage 1 Training Configuration
#
# SAFETY TIP: Before modifying this file, create a backup:
#   cp configs/default_config.yaml configs/backup/default_config_$(date +%Y%m%d).yaml
#
# Or use the built-in backup features:
#   python train_from_config.py --config configs/default_config.yaml \
#     --save-config configs/backup/default_config_backup.yaml

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Model style determines the type of model and training approach
  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
  style: "stage_1"
  
  # Core transformer parameters
  d_model: 768              # Model dimension (hidden size)
  n_heads: 12               # Number of attention heads
  n_layers: 12              # Number of transformer layers
  max_len: 2048             # Maximum sequence length
  dropout: 0.1              # Dropout rate
  ff_mult: 4                # Feedforward multiplier (ff_dim = d_model * ff_mult)
  reference_model_seed: 42  # Seed for model initialization
  
  # Block type for first transformer block
  # Options: "self_attention", "geometric_attention", "reflexive_attention", "self_consensus"
  block_type: "self_consensus"
  
  # Block-specific parameters
  block_params:
    # Self-Consensus parameters (used when block_type is "self_consensus")
    num_iterations: 1       # Number of consensus gradient iterations
    connectivity_type: "local_window"  # "local_window" or "top_w"
    w: 2                    # Window size for local_window, or number of connections for top_w
    r: 24                   # Rank of Lambda_ij matrices
    edge_hidden_dim: 12     # Hidden dimension for edge networks
  
  # FSQ parameters (required for stage_1)
  fsq:
    latent_dim: 32          # Pre-quantized continuous latent dimension
    levels: [7, 5, 5, 5, 5] # Quantization levels for each dimension

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Basic training parameters
  batch_size: 32
  max_epochs: 100
  learning_rate: 1e-4
  
  # Data paths
  data_dir: "sample_data/1k"
  checkpoint_dir: "checkpoints"

# =============================================================================
# Loss Configuration
# =============================================================================
loss:
  # Loss type: "cross_entropy", "kabsch_rmsd", "score_entropy"
  type: "kabsch_rmsd"  # FSQ stage 1 uses RMSD loss

# =============================================================================
# Masking Configuration
# =============================================================================
masking:
  # Masking strategy: "simple", "complex", "discrete_diffusion", "none"
  strategy: "simple"
  
  # Simple masking parameters
  simple:
    mask_prob_seq: 0.2      # Probability of masking sequence tokens
    mask_prob_struct: 0.2   # Probability of masking structure tokens
  
  # Complex masking parameters
  # complex:
  #   # Add parameters when ComplexMaskConfig is fully defined
  
  # Discrete diffusion parameters (not used for stage_1)
  # discrete_diffusion:
  #   noise_schedule: "linear"  # "linear", "inverted_u", or "uniform"
  #   sigma_min: 0.31
  #   sigma_max: 5.68
  #   num_timesteps: 100