# Example Configuration with Safety Best Practices
# ================================================
#
# This configuration file demonstrates best practices for using
# the Odyssey configuration system's safety features.
#
# BEFORE MODIFYING: Always create a backup!
# Method 1: Manual backup
#   cp configs/example_with_safety_comments.yaml configs/backup/$(date +%Y%m%d)_example.yaml
#
# Method 2: Using built-in features
#   python train_from_config.py --config configs/example_with_safety_comments.yaml \
#     --save-config configs/backup/my_experiment.yaml
#
# Method 3: Convert to JSON for version control
#   python configs/save_config_to_json.py --config configs/example_with_safety_comments.yaml

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Model style - this determines which configuration class will be used
  # Safety: The configuration system will validate this matches the required parameters
  style: "mlm"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
  
  # Core transformer parameters
  # Safety: These values are automatically backed up in _config_dict
  d_model: 512              # Model dimension
  n_heads: 8                # Number of attention heads
  n_layers: 8               # Number of transformer layers
  max_len: 1024             # Maximum sequence length
  dropout: 0.1              # Dropout rate
  ff_mult: 4                # Feedforward multiplier
  reference_model_seed: 42  # For reproducibility
  
  # Block configuration
  # Safety: Block configs are automatically serialized when saving to JSON
  block_type: "self_consensus"
  block_params:
    num_iterations: 2
    connectivity_type: "local_window"
    w: 3
    r: 16
    edge_hidden_dim: 8
  
  # Required for MLM style
  # Safety: Path existence is validated during configuration loading
  fsq_encoder_path: "checkpoints/stage1/fsq_encoder.pt"

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Training hyperparameters
  # Safety: Command-line overrides are merged safely without modifying the file
  batch_size: 64
  max_epochs: 50
  learning_rate: 5e-4
  
  # Data paths
  # Safety: Relative paths are automatically resolved to absolute paths
  data_dir: "sample_data/1k"
  checkpoint_dir: "checkpoints"

# =============================================================================
# Loss Configuration
# =============================================================================
loss:
  # Loss type must match model style
  # Safety: Configuration validation ensures consistency
  type: "cross_entropy"  # MLM uses cross_entropy
  
  # Loss weights
  # Safety: These nested parameters are preserved in JSON serialization
  weights:
    sequence: 1.0
    structure: 1.0
  
  # Which elements contribute to loss
  loss_elements: "masked"  # Only masked positions for MLM

# =============================================================================
# Masking Configuration
# =============================================================================
masking:
  # Masking strategy
  # Safety: Invalid strategies are caught during configuration creation
  strategy: "simple"
  
  # Simple masking parameters
  # Safety: Probabilities are validated to be between 0 and 1
  simple:
    mask_prob_seq: 0.15      # Standard MLM masking rate
    mask_prob_struct: 0.15   # Same rate for structure

# =============================================================================
# Safety Tips and Recovery Options
# =============================================================================
# 
# 1. BACKUP BEFORE EXPERIMENTS:
#    Before running experiments with modified parameters:
#    python configs/save_config_to_json.py --config configs/example_with_safety_comments.yaml \
#      --output configs/backup/experiment_$(date +%Y%m%d_%H%M%S).json
#
# 2. TRACK CONFIGURATION CHANGES:
#    The configuration system preserves original values even if modified:
#    - Original values stored in config._config_dict
#    - Access with: config.get_config_dict()
#
# 3. SAVE WITH CHECKPOINTS:
#    When saving model checkpoints, include configuration:
#    torch.save({
#        'model_state_dict': model.state_dict(),
#        'model_config': model_config.get_config_dict(),
#        'training_config': training_config.get_config_dict()
#    }, checkpoint_path)
#
# 4. RECOVER FROM JSON:
#    If you have a JSON backup:
#    from odyssey.src.configurations import Config
#    config = Config.load_from_json('configs/backup/my_config.json')
#
# 5. VERSION CONTROL:
#    Add meaningful backups to git:
#    git add configs/backup/meaningful_experiment_name.json
#    git commit -m "Configuration for experiment X with changes Y"
#
# Remember: The configuration system is designed to prevent accidental loss
# of configuration data. Use these features to ensure reproducibility!