# Odyssey Default Configuration File
# FSQ Stage 2 Training Configuration

# =============================================================================
# Model Configuration
# =============================================================================
model_cfg:
  type: "fsq_cfg"  # Options: "fsq_cfg", "trunk_cfg"
  params:
    style: "stage_2"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
    
    # Core transformer parameters
    d_model: 128
    n_heads: 1
    n_layers: 10
    max_len: 2048
    dropout: 0.1
    ff_mult: 4
    reference_model_seed: 42
    latent_dim: 32
    fsq_levels: [7, 5, 5, 5, 5]
    fsq_encoder_path: "/workspace/demo/Odyssey/checkpoints/fsq/SC_stage_1_simple_model.pt"  # Required for stage_2
    
    # First block configuration
    first_block_cfg:
      type: "self_consensus_cfg"  # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
      params:
        consensus_num_iterations: 1
        consensus_connectivity_type: "local_window"  # Options: "local_window", "top_w"
        consensus_w: 2
        consensus_r: 24
        consensus_edge_hidden_dim: 12
      # type: "self_attention_cfg"
      # params: {}
      # type: "geometric_attention_cfg"
      # params: {}
      # type: "reflexive_attention_cfg"
      # params: {}

# =============================================================================
# Training Configuration
# =============================================================================
train_cfg:
  type: "training_cfg"
  params:
    batch_size: 4
    max_epochs: 50
    learning_rate: 0.0001
    data_dir: "/workspace/demo/Odyssey/sample_data/1k.csv"
    checkpoint_dir: "/workspace/demo/Odyssey/checkpoints/fsq"
    
    # Loss configuration
    loss_config:
      type: "kabsch_rmsd_loss_cfg"  # Options: "cross_entropy_loss_cfg", "kabsch_rmsd_loss_cfg", "score_entropy_loss_cfg"
      params: {}
    
    # Masking configuration
    mask_config:
      type: "no_mask_cfg"
      params: {}