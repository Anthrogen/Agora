# Configuration for FSQ Stage 1 training with Geometric Attention
# Example of using different attention blocks

model_cfg:
  type: "fsq_cfg"
  params:
    style: "stage_1"
    d_model: 256
    n_heads: 8
    n_layers: 8
    max_len: 1024
    dropout: 0.1
    ff_mult: 4
    reference_model_seed: 42
    
    # Using Geometric Attention instead of Self Attention
    first_block_cfg:
      type: "geometric_attention_cfg"
      params: {}
    
    latent_dim: 32
    fsq_levels: [7, 5, 5, 5, 5]
    fsq_encoder_path: null

train_cfg:
  type: "training_cfg"
  params:
    batch_size: 8
    max_epochs: 100
    learning_rate: 5e-5
    data_dir: "sample_data/1k"
    checkpoint_dir: "checkpoints/fsq"
    
    loss_config:
      type: "kabsch_rmsd_loss_cfg"
      params: {}
    
    mask_config:
      type: "simple_mask_cfg"
      params:
        mask_prob_seq: 0.15
        mask_prob_struct: 0.15