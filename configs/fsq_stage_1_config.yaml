# Odyssey Default Configuration File
# FSQ Stage 1 Training Configuration

# =============================================================================
# Model Configuration
# =============================================================================
model_cfg:
  autoencoder_cfg:  # Options: "autoencoder_cfg", "trunk_cfg"
    style: "stage_1"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
    autoencoder_path: null
    reference_model_seed: 42

    vocab_per_residue_path: "/workspace/demo/Odyssey/odyssey/train/vocab_per_residue_annotations.txt"
    vocab_global_path: "/workspace/demo/Odyssey/odyssey/train/vocab_global_annotations.txt"
    max_annotations_per_residue: 4
    max_len_global: 512

    encoder_cfg:
      encoder_cfg:
        latent_dim: 32
        fsq_levels: "7x5x5x5x5"
        d_model: 128
        n_heads: 1
        n_layers: 3
        max_len: 2048
        dropout: 0.1
        ff_mult: 4
        context_cfg: null  # Not needed for stage_1

        # First block configuration
        first_block_cfg:
          self_consensus_cfg:   # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"  # Options: "local_window", "scored_window"
            consensus_w: 2
            consensus_r: 24
            consensus_edge_hidden_dim: 12
          # - self_attention_cfg: {}
          # - geometric_attention_cfg: {}
          # - reflexive_attention_cfg: {}

    decoder_cfg:
      decoder_cfg:
        latent_dim: 32
        fsq_levels: "7x5x5x5x5"
        d_model: 128
        n_heads: 1
        n_layers: 3
        max_len: 2048
        dropout: 0.1
        ff_mult: 4
        context_cfg: null  # Not needed for stage_1

        # First block configuration
        first_block_cfg:
          self_consensus_cfg:   # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"  # Options: "local_window", "scored_window"
            consensus_w: 2
            consensus_r: 24
            consensus_edge_hidden_dim: 12
          # - self_attention_cfg: {}
          # - geometric_attention_cfg: {}
          # - reflexive_attention_cfg: {}

# =============================================================================
# Training Configuration
# =============================================================================
train_cfg:
  training_cfg:
    batch_size: 4
    max_epochs: 50
    checkpoint_freq: null
    max_steps_val: null
    data_dir: "/workspace/demo/Odyssey/sample_data/3k.csv"
    checkpoint_dir: "/workspace/demo/Odyssey/checkpoints/fsq"
    jump_start: null # "/workspace/demo/Odyssey/checkpoints/fsq/fsq_stage_1_config/fsq_stage_1_config_000/model.pt"

    # Optimizer schedule configuration.
    optim_schedule_config:
      # linear_decay_scheduler_cfg:
      #   base_learning_rate: 0.00025
      #   min_learning_rate: 0.00005
      #   num_epochs_decay: 100000
      #   num_epochs_warmup: 5000
      flat_scheduler_cfg:
        learning_rate: 0.00005
    
    # Loss configuration.
    loss_config:
      kabsch_rmsd_loss_cfg: {}  # Options: "cross_entropy_loss_cfg", "kabsch_rmsd_loss_cfg", "score_entropy_loss_cfg"
    
    # Masking configuration
    mask_config:
      - simple_mask_cfg:
          mask_prob_seq: 0.2
          mask_prob_struct: 0.2
      - complex_mask_cfg: {}
      - diffusion_mask_cfg:
          noise_schedule: "uniform"
          sigma_min: 0.31
          sigma_max: 5.68
          num_timesteps: 100
          corruption_mode: "absorb"