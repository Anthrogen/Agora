# Odyssey Default Configuration File
# Transformer Trunk Discrete Diffusion Training Configuration

# =============================================================================
# Model Configuration
# =============================================================================
model_cfg:
  trunk_cfg:
    style: "discrete_diffusion"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
    autoencoder_path: "/workspace/demo/Odyssey/checkpoints/fsq/fsq_stage_2_config/fsq_stage_2_config_002/checkpoint_step_26316.pt"
    reference_model_seed: 42

    vocab_domains_path: "/workspace/demo/Odyssey/odyssey/train/vocab_domains.txt"
    vocab_orthologous_groups_path: "/workspace/demo/Odyssey/odyssey/train/vocab_orthologous_groups.txt"
    vocab_semantic_description_path: "/workspace/demo/Odyssey/odyssey/train/vocab_semantic_descriptions.txt"
    max_domains_per_residue: 4
    max_len_orthologous_groups: 512
    max_len_semantic_description: 128

    transformer_cfg:
      transformer_cfg:
        d_model: 128
        n_heads: 1
        n_layers: 3
        max_len: 2048
        dropout: 0.1
        ff_mult: 4

        context_cfg:
          cross_consensus_cfg:  # Options: "cross_attention_cfg", "cross_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"
            consensus_w: 2
            consensus_r: 24
            consensus_edge_hidden_dim: 12
          # cross_attention_cfg: {}

        # First block configuration
        first_block_cfg:
          self_consensus_cfg:  # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"  # Options: "local_window", "scored_window"
            consensus_w: 2
            consensus_r: 24
            consensus_edge_hidden_dim: 12
          # self_attention_cfg: {}
          # geometric_attention_cfg: {}
          # reflexive_attention_cfg: {}

# =============================================================================
# Training Configuration
# =============================================================================
train_cfg:
  training_cfg:
    batch_size: 4
    max_epochs: 100
    checkpoint_freq: null
    max_steps_val: null
    data_dir: "/workspace/demo/Odyssey/sample_data/3k.csv"
    checkpoint_dir: "/workspace/demo/Odyssey/checkpoints/transformer_trunk"
    jump_start: null

    # Optimizer schedule configuration.
    optim_schedule_config:
      # linear_decay_scheduler_cfg:
      #   base_learning_rate: 0.00025
      #   min_learning_rate: 0.00005
      #   num_epochs_decay: 100000
      #   num_epochs_warmup: 5000
      flat_scheduler_cfg:
        learning_rate: 0.00005
      # decay_scheduler_cfg:
      #   base_learning_rate: 0.0002
      #   min_learning_rate: 0.00005
      #   num_epochs_decay: 70000
    
    # Loss configuration
    loss_config:
      score_entropy_loss_cfg: # Options: "cross_entropy_loss_cfg", "kabsch_rmsd_loss_cfg", "score_entropy_loss_cfg"
        seq_loss_weight: 1.0
        struct_loss_weight: 1.0
    
    # Masking configuration
    mask_config:
      diffusion_mask_cfg:
        noise_schedule: "uniform"
        sigma_min: 0.31
        sigma_max: 5.68
        num_timesteps: 100
        corruption_mode: "absorb"
