# Odyssey Default Configuration File
# Transformer Trunk MLM Training Configuration

# =============================================================================
# Model Configuration
# =============================================================================
model_cfg:
  trunk_cfg:
    style: "mlm"  # Options: "stage_1", "stage_2", "mlm", "discrete_diffusion"
    autoencoder_path: "/workspace/demo/Odyssey/checkpoints/fsq/fsq_stage_1_config/fsq_stage_1_config_000/model.pt"
    reference_model_seed: 42

    vocab_per_residue_path: "/workspace/demo/Odyssey/odyssey/train/vocab_per_residue_annotations.txt"
    vocab_global_path: "/workspace/demo/Odyssey/odyssey/train/vocab_global_annotations.txt"
    max_annotations_per_residue: 4
    max_len_global: 512

    transformer_cfg:
      transformer_cfg:
        d_model: 1024
        n_heads: 8
        n_layers: 16
        max_len: 2048
        dropout: 0.1
        ff_mult: 4

        context_cfg:
          cross_consensus_cfg:  # Options: "cross_attention_cfg", "cross_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"
            consensus_w: 2
            consensus_r: 12
            consensus_edge_hidden_dim: 12
          # cross_attention_cfg: {}

        # First block configuration
        first_block_cfg:
          self_consensus_cfg:  # Options: "self_attention_cfg", "geometric_attention_cfg", "reflexive_attention_cfg", "self_consensus_cfg"
            consensus_num_iterations: 1
            consensus_connectivity_type: "local_window"  # Options: "local_window", "scored_window"
            consensus_w: 2
            consensus_r: 12
            consensus_edge_hidden_dim: 12
          # self_attention_cfg: {}
          # geometric_attention_cfg: {}
          # reflexive_attention_cfg: {}

# =============================================================================
# Training Configuration
# =============================================================================
train_cfg:
  training_cfg:
    batch_size: 4
    max_epochs: 5
    checkpoint_freq: 10000
    max_steps_val: 500
    data_dir: "/workspace/demo/Odyssey/sample_data/3k.csv"
    checkpoint_dir: "/workspace/demo/Odyssey/checkpoints/transformer_trunk"
    jump_start: "/workspace/demo/Odyssey/checkpoints/transformer_trunk/mlm_config/mlm_config_000/model.pt"

    # Optimizer schedule configuration.
    optim_schedule_config:
      # linear_decay_scheduler_cfg:
      #   base_learning_rate: 0.00025
      #   min_learning_rate: 0.00005
      #   num_epochs_decay: 100000
      #   num_epochs_warmup: 5000
      flat_scheduler_cfg:
        learning_rate: 0.00001
      # decay_scheduler_cfg:
      #   base_learning_rate: 0.0002
      #   min_learning_rate: 0.00005
      #   num_epochs_decay: 70000
    
    # Loss configuration
    loss_config:
      cross_entropy_loss_cfg:
        seq_loss_weight: 1.0
        struct_loss_weight: 1.0
        loss_elements: "masked"  # Different loss element strategy
    
    # Masking configuration
    mask_config:
      # simple_mask_cfg:
      #   mask_prob_seq: 0.2
      #   mask_prob_struct: 0.2
      complex_mask_cfg: {}
